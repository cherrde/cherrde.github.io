<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
    <link rel="shortcut icon" href="./favicon.ico" />
  <style>
      body{
        font-family: Georgia, 'Times New Roman', Times, serif;
    }
    article{
        margin-top: 5em;
    }
    .side-nav{
        color: black;
        border-radius: 50%;
        text-decoration: none;
        border: none;
        width: 3em;
        height: 3em;
        padding: 10px;
        background-color: white;
        transition: 0.5s;
        margin-bottom: 10px;
    }
    .side-nav-div{
        margin-top: 7em;
        text-align: center;
    }
    .side-nav:hover{
        color: white;
        background-color: black;
    }

    html {
        font-size: 1.3rem;
      }
      blockquote {
    background: #f9f9f9;
    border-left: 10px solid #ccc;
    margin: 1.5em 10px;
    padding: 0.5em 10px;
    quotes: "\201C""\201D""\2018""\2019";
  }
  blockquote:before {
    color: #ccc;
    content: open-quote;
    font-size: 4em;
    line-height: 0.1em;
    margin-right: 0.25em;
    vertical-align: -0.4em;
  }
  blockquote p {
    display: inline;
  }

@media (max-width:1000px) {
    .row{
        flex-direction: column;
    }
    .col-1{
        width: 100%;
        margin-top: 10px;
        float: left;
    }
    .col-11{
        margin-top: 4px !important;
        width: 100%;
    }
}

  </style>

</head>
<body class='container'>
    <div class='row'>
    <div class='col-1 side-nav-div'>
        <button onclick="window.location.replace('https://example.com')" class='side-nav'><span class="fa fa-home"></span></button>
        <button onclick="increaseFontSize()" class='side-nav'>A+</button>
        <button onclick="decreaseFontSize()" class='side-nav'>A-</button>
    </div>
    <article class='col-11 mt-5'>
      <figure>
        <img src="images/coffee.jpg" alt="Header Image" width="474" height="355">
        <figcaption>Image Public Domain</figcaption>
      </figure>
      <h1>Starbucks Capstone</h1>
      <h2>Starbucks: successful mobile offers and their characteristics</h2>
      <h3>(Or Introduction)</h3>
        <p>For the Starbucks Capstone project we are provided a set of "data that mimics customer
        behavior on the Starbucks rewards mobile app." This data set includes information about people,
        transactions, offers and events related to those events. Some of the offers in the data are
        discounts, some are informational and some are buy one get one (bogo) offers. The task provided
        was to cleanse and combine the data set into some useful form and then determine some information
        about this data.</p>
      <h2>Project Overview and Problem statement:</h2>
        <h3>(i.e.) What question are we asking?</h3>
          <p>I will focus on two specific questions. What types of offers are most successful and will drive
          more business and what features or characteristics will drive successful offers?</p>
          <p>So a two fold question:</p>
            <ol>
              <li>Which offers are the most successful</li>
              <li>Which features lead to successful offers?</li>
            </ol>
          <p>Why are these questions important? The first will allow Starbucks to target the types of offers that are most
          likely to be successful and generate revenue based on that. The second question will allow us to target
          those offers to those most likely to participate and again, generate revenue.</p>
        <h3>Methodology/Strategy</h3>
          <p>My plan is to first cleanse the data and merge it. As I'm not certain what each data set will look like when
          merged, this may be an iterative process. Then I will review the data and see if I can find which offers tend to
          be successful and if there are characterizations of those offers in the demographic data. I will use
          visualizations when possible to help illustrate the data. </p>
      <h2>Exploratory Data Analysis and Cleansing with Visualizations</h2>
        <p>I first began by looking at the profile data, data about the people. There were more men than women.</p>
        <br>
          <img src="images/gender.png" alt="Header Image">
        </br>
        <p>The women, generally, were more evenly distributed over teh income levels, though, with men tending to be right skewed.</p>
        <br>
          <img src="images/gender_income.png" alt="Header Image">
        </br>
        <p>I generated histograms of the age, membership year and income to see how they looked.</p>
        <br>
          <img src="images/demographics.png" alt="Header Image">
        </br>
        <p>I noticed a significant outlier on the age (at age 118, this is noted as the age number used when a customer didn't enter an age).
        Most members were skewed to the left (newer memberships) and income was left skewed, with peaks in the 50 to 70 thousand per year range</p>
        <p>I then began working on the merging the data. I was able to create a data set that allowed me to look at the look at the data a bit more
        holistically. I reviewed the types of events and how many offers of each type were sent out. I viewed information about the transactions (mean
        amount of $13.68, for example). I found I wanted to know more about the offers and the events associated. How many offers were recieved, viewed
        and then completed?</p>
        <br>
          <img src="images/offer_by_event.png" alt="Header Image">
        </br>
        <p>I knew, in determining successful offers, I didn't really need the transaction data at this point, and I also knew I needed to "flatten" my
        data (combine the recieved, viewed and complete values into a person by offer grouping). I needed to ensure that an offer was really acted on, rather
        than coincidentally being compelted. That meant ensuring that each offer completed was actually recieved and viewed and then completed within the timeframe
        alotted. I created a function to do that. From there I could see which offers truly were "successful".</p>
        <br>
           <img src="images/successful_offers.png" alt="Header Image">
        </br>
        <p>I found two offers really stood out the discount_10_10_2 and discount 7_7_3. These were followed by bogo_5_5_5 and bogo 5_7_5. These discount offers
        were the lower difficulty offers (7 or 10, vs. 20) and if there were similar difficulties and reward (10 vs. 10) the longer duration won out. <thead>
        same seems to hold true for the bogo (buy one get one) offers.</p>
        <p>I reviewed the demographic data of these successful offers, but found the data looked very similar to our source material. The gender, membership year
        and income histograms seemed very simliar. There had to be a better way to get at the characterstics that determined these successful offers. And of course,
        there is, classifying models.</p>
      <h2>Data Preprocessing</h2>
        <p>Some final feature engineering and data preprocessing was conducted. First we dropped unnecessary columns: person, person_id, offer_id, and offer_name.
        Then I broke the data into our offer (our target set) and feature sets. Next, we converted several numerical values (income, became_member_on) to categorical
        values using pandas getdummies functionality. This would allow us to include them in our classifier engine later more easily. I also removed null values
        (since our classifier engines can't use them). I then used a MinMaxScaler to scale numeric variables we didn't want to turn into categories (time, difficulty,
        duration, age and reward)</p>
      <h2>Metrics: Next Steps, modeling and determinant features</h2>
         <p>The first step was to get information about a basic Naive predictor and calculate the Accuracy and F1 Score for comparison to models I build later.
         Why use Accuracy and F1 Score? Accuracy, of course is good since it high accuracy means it is easy to pinpoint predictions that will be successful.
         Since we're note overly concerned about false positives and false negatives, this will be a good measure. Additionally, I will also use the F1 score
         since it will provide more information about recall and precision. F1 is a weighted average of the precision and recall so if one or the other gets
         out of balance this helps keep our predictions on track. I will compare those scores with three more classifiers to get a good solid base to work from.</p>
      <h3>Model Evaluation and Validation: Models to Try</h3>
        <p>The data we have is categorical. Using the following link (https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) I reviewed which
        model might make the most sense. I decided on the following three: Support Vector Machine, K-Nearest Neighbor and the Bagging Classifier method. I investigated
        and experimented with ADA Boost, Decision Trees and Random Forest Classifiers as well. We'll train on three different amounts of the data (1%, 10% and all) we'll
        compare accuracy and f-scores to see which models seem the best for us. Hyperparameter settings were left at their defaults. For the SVC Classifier we used the default
        setting of degree 3 and random_state of 0 for reproducible output. For KNeighbors we stuck with 5 nieghbors, auto select on the algorithm, leaf size of 30
        random_state of 0, again for reproducibility, and uniform weights. For the Bagging Classifier this included 10 base estimators, random_state of 0 to ensure
        each run was the same.</p>
      <h3>Refinement and Justification: Model Results</h3>
        <p>I modeled a few different classifiers starting with a Naive Predictor. The Accuracy was about 45% and F1 Score 0.51 (middle of the road, not very good). I then
        tested the models discussed above to find the a good solid model that would predict at a high level. The SVC model had an Accuracy in the testing set between 77%
        and 80% and an F1 Score range from 0.75 to 0.76. Pretty solid, much better than the Naive predictor. The KNeighbors ranged from 72% to 77%, not as good with
        F1 Scores from .70 to .75. Finally the Bagging Classifier ranged from 75% to 80% with F1 Scores from .74 to .79. The Bagging Classifier was marginally better
        than the SVC, but the final determinant was the speed. The Bagging Classifier was much, much faster taking significantly less time to deliver results than the
        SVC with all the data used in the Training and Testing sets. Hyperparameters were left at defaults since these worked well enough to reach satisfactory results.
        With the Bagging Classifier I did explore changing the n_estimators to 5 and 20 with limited effect. Then looking at max_features of 5 and 15 and those produced
        both less accruate and lower f1 score results. I tried bootstrapping the features set to True, this did slightly improve the accuracy and f1 score. So future
        investigations might explore that.</p>
        <br>
          <img src="images/model_analysis.png" alt="Header Image">
        </br>
        <p>With an identified model, I then used that  to determine the features that would be most important using feature_importances_. This resulted in age, time, reward,
        difficulty and duration. Which leads me to conclude that the type of offer, combined with its rewards are major factors in whether it will be successful. Additionally
        the age of the respondent as well as what appears to be newer members (2018 and 2016) figured significantly in our top ten predictive features.</p>
        <br>
          <img src="images/pred_features.png" alt="Header Image">
        </br>
      <h2>Conclusion</h2>
        <h4>Enjoy that Discounted Cuppa Joe/Discounts are successful</h4>
        <p>I started out with two main questions "What types of offers are most successful and will drive more business and what features will drive successful offers?" To get
        a better understanding of that we reviewed the data, manipulated and visualized it in a few different ways. We combined it and formatted it then created a solid
        feature set. We were able to view that data and determine a few different offers that tended to be successful (discount_10_10_2, discount 7_7_3 and bogo_5_5_5 and
        bogo 5_7_5). We created some visualizations of the demographics for those offers, but it wasn't very helpful.</p>
        <h4>Focus on the offer characteristics, but also consider age of the target audience and newer members</h4>
        <p>So we created and compared some classifying models. We found the Bagging Classifier provided the fastest, yet also accurate and precise results. With that model we were
        able to determine our predictive or determinant features (age, time, reward, difficulty, duration) and some additional (like newer membership and income level of 60k).</p>
      <h2>Improvements</h2>
        <p>This isn't the end, though. We could improve the models by gathering more data and rerunning. We could use the SVC with a more powerful system (potenially pushing it off
        to a high end graphics engine to improve performance). We could delve into changing the various hyperparameters of our models to increase our accuracy and f1 score. We could
        look at our modeling from a different perspective and create a recommendation engine.</p>
        <br></br>
    </article>

<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
    function increaseFontSize(){

        let html = document.getElementsByTagName('html')[0];
        let currentFont = window.getComputedStyle(html, null).getPropertyValue("font-size");
        if(currentFont == '35px'){
            return false;
        }

        html.style.fontSize = parseInt(currentFont) + parseInt(3) + "px";
    }
    function decreaseFontSize(){

        let html = document.getElementsByTagName('html')[0];
        let currentFont = window.getComputedStyle(html, null).getPropertyValue("font-size");
        if(currentFont == '14px'){
            return false;
        }
        html.style.fontSize = parseInt(currentFont) - parseInt(3) + "px";
    }

  function addDarkmodeWidget() {
    const options = {
        time: '0.5s', // default: '0.3s'
        mixColor: '#fff', // default: '#fff'
        backgroundColor: '#fff',  // default: '#fff'
        buttonColorDark: '#100f2c',  // default: '#100f2c'
        buttonColorLight: '#fff', // default: '#fff'
        saveInCookies: false, // default: true,
        label: 'ðŸŒ™', // default: ''
        autoMatchOsTheme: true // default: true
        }

        const darkmode = new Darkmode(options);
        darkmode.showWidget();
  }
  window.addEventListener('load', addDarkmodeWidget);
</script>

</body>
</html>
